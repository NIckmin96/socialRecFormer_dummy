{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "data_dir = os.path.join(os.getcwd(),'dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ciao_dir = os.path.join(data_dir,'ciao')\n",
    "ciao_data_dir = os.path.join(ciao_dir,'rating.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "ciao_data = loadmat(ciao_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "ciao_df = pd.DataFrame(ciao_data['rating'], columns=['user_id','product_id','category_id','rating', 'helpfulness'])\n",
    "ciao_df = ciao_df.drop(columns=['category_id','helpfulness'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  product_id  rating\n",
       "0        1           1       3\n",
       "1        1           2       4\n",
       "2        1           3       4\n",
       "3        1           4       5\n",
       "4        1           5       4"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(ciao_df.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 사용하지 않는 columns\n",
    "- Ciao\n",
    "    - category_id, helpfulness\n",
    "- Epinions\n",
    "    - category_id, helpfulness, timestamp\n",
    "\n",
    "## Sequence 정보를 drop하는 것에 대해\n",
    "SocialTransformer\n",
    "- sequence 정보 사용하지 않음\n",
    "    - Epinions에는 timestamp 존재\n",
    "    - Positional encoding 대신, centrality degree embedding 추가\n",
    "    - 각 노드가 어느 정도의 영향력을 가지고 있는지에 대한 정보 제공\n",
    "- sequence 정보가 없을 경우 생길 수 있는 문제점 \n",
    "    - trend의 영향력을 간과할 수 있음\n",
    "        - t시점의 interaction과 t+10 시점의 interaction을 동일 선상에 놓고 모델링하는 것은 비효율적일 수 있음\n",
    "- **Sequence 정보에 대해서는 추후 더 생각해보기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trust_file_ciao = loadmat(os.path.join(ciao_dir,'trustnetwork.mat'))\n",
    "trust_df_ciao = pd.DataFrame(trust_file_ciao['trustnetwork'], columns=['user_id_1', 'user_id_2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id_1</th>\n",
       "      <th>user_id_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id_1  user_id_2\n",
       "0          1          2\n",
       "1          1          3\n",
       "2          1          4\n",
       "3          1          5\n",
       "4          1          6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(trust_df_ciao.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "trust_df\n",
    "- social interaction 관계를 나타내는 데이터"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "networks(nx)\n",
    "- from_pandas_edgelist\n",
    "    - pandas dataframe을 인자로 받아 네트워크 관계를 구성해줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset and filter data(args : rating df, trust df)\n",
    "# nx(networks) : 그래프 데이터 라이브러리(source-target 으로 연결됨)\n",
    "import networkx as nx\n",
    "social_network = nx.from_pandas_edgelist(trust_df_ciao, source='user_id_1', target='user_id_2')\n",
    "social_ids = list(set(social_network.nodes))\n",
    "user_item_ids = ciao_df['user_id'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setxor1d : 두개의 arr 사이에서 겹치지 않는 부분 추출\n",
    "non_users = np.setxor1d(social_ids, user_item_ids)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "공통되지 않는 유저는 학습 데이터에 사용할 수 없으므로 제거\n",
    "- $\\because$ user-item / user-user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ciao_df = ciao_df[~ciao_df['user_id'].isin(non_users)]\n",
    "ciao_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user/item mapping table -> 라벨링 하는 부분인듯?\n",
    "mapping_table_user = {user_id:idx+1 for idx,user_id in enumerate(social_ids)}\n",
    "mapping_table_item = {item_id:idx+1 for idx,item_id in enumerate(ciao_df['product_id'].unique())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ciao_df['user_id'] = ciao_df['user_id'].map(mapping_table_user)\n",
    "ciao_df['product_id'] = ciao_df['product_id'].map(mapping_table_item)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`reset_and_filter_data`\n",
    "1. user-item 데이터와 user-user 데이터 비교해, 공통되는 부분만 남김(필터링)\n",
    "2. 필터링 후, user_id / item_id에 대해 라벨링 진행(1부터 연속적으로) & id 변경"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rating matrix(sparse matrix)\n",
    "- 0번쨰 row/column 비어있도록 설계되어있음\n",
    "    - zero padding 때문에?\n",
    "    - 0번째 user/item은 비어있는 sequence 채울떄 사용 -> 정상적인 sequence에 들어가면 안됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "# user-item rating matrix 만들기\n",
    "# number of user x number of item\n",
    "rating_matrix = sparse.lil_matrix((ciao_df['user_id'].max()+1, ciao_df['product_id'].max()+1))\n",
    "# 채워넣기\n",
    "for index in ciao_df.index:\n",
    "        rating_matrix[ciao_df['user_id'][index], ciao_df['product_id'][index]] = ciao_df['rating'][index]\n",
    "\n",
    "rating_matrix = rating_matrix.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle data\n",
    "from sklearn.utils import shuffle\n",
    "ciao_df = shuffle(ciao_df, random_state=42)\n",
    "num_test = int(len(ciao_df)*0.1)\n",
    "rating_test_set = ciao_df.iloc[:num_test]\n",
    "rating_valid_set = ciao_df.iloc[num_test:num_test*2]\n",
    "rating_train_set = ciao_df.iloc[num_test*2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate social dataset\n",
    "users  = rating_train_set.user_id.unique().tolist()\n",
    "trust_df = trust_df_ciao\n",
    "social_graph = trust_df[(trust_df['user_id_1'].isin(users))&(trust_df['user_id_2'].isin(users))]\n",
    "# social graph -> trustnetwork_train_seed_42.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate user degree\n",
    "social_graph = nx.from_pandas_edgelist(social_graph, source='user_id_1', target='user_id_2')\n",
    "user_degree = {node:degree for node,degree in social_graph.degree()}\n",
    "user_degree = pd.DataFrame(user_degree.items(), columns=['user_id','degree'])\n",
    "user_degree = user_degree.sort_values(by='user_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate item degree\n",
    "rating_df = rating_train_set\n",
    "item_degree = rating_df.groupby('product_id')['user_id'].nunique().reset_index().sort_values(by='product_id')\n",
    "item_degree.columns = ['product_id','degree']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate interacted items table\n",
    "item_degree_dict = dict(zip(item_degree['product_id'], item_degree['degree']))\n",
    "user_item_df = rating_df.groupby('user_id').agg({'product_id':list, 'rating':list}).reset_index()\n",
    "user_item_df['product_degree'] = user_item_df['product_id'].apply(lambda x: [item_degree_dict[id] for id in x])\n",
    "empty_data = [0, [0 for _ in range(4)], [0 for _ in range(4)], [0 for _ in range(4)]]\n",
    "user_item_df.loc[-1] = empty_data\n",
    "user_item_df.index = user_item_df.index+1\n",
    "user_item_df = user_item_df.sort_values(by='user_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate interacted users table\n",
    "item_user_df = rating_df.groupby('product_id').agg({'user_id':list, 'rating':list}).reset_index()\n",
    "item_user_df = user_item_df.sort_values(by='product_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate social random walk sequence\n",
    "all_path_list = []\n",
    "anchor_nodes = np.random.choice(social_graph.nodes(), size=1000, replace=False)\n",
    "anchor_nodes = np.repeat(anchor_nodes,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_next_node(input_G, previous_node, current_node, RETURN_PARAMS):\n",
    "    \"\"\"\n",
    "    input_G의 current_node에서 weight를 고려하여 다음 노드를 선택함. \n",
    "    - 이 과정에서 RETURN_params를 고려함. \n",
    "    - 이 값은 previous_node로 돌아가는가 돌아가지 않는가를 정하게 됨. \n",
    "    \"\"\"\n",
    "        \n",
    "    select_probabilities = {}\n",
    "    \n",
    "    for node in input_G.neighbors(current_node):\n",
    "        if node != previous_node:\n",
    "            select_probabilities[node] = 1   \n",
    "        \n",
    "    select_probabilities_sum = sum(select_probabilities.values()) # len selected_probabilities\n",
    "    select_probabilities = {k: v/select_probabilities_sum*(1-RETURN_PARAMS) for k, v in select_probabilities.items()}\n",
    "    if previous_node is not None:\n",
    "        select_probabilities[previous_node]=RETURN_PARAMS # 이 노드는 RETURN_PARAMS에 의해 결정됨. \n",
    "    \n",
    "    # print(select_probabilities)\n",
    "    # print(select_probabilities_sum)\n",
    "    \n",
    "    if select_probabilities_sum == 0:\n",
    "        return 0\n",
    "    \n",
    "    selected_node = np.random.choice(\n",
    "        a=[k for k in select_probabilities.keys()],\n",
    "        p=[v for v in select_probabilities.values()]\n",
    "    )\n",
    "    return selected_node"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- walk length==0(두번쨰 sequence)\n",
    "    - prob dictionary(select_probabilities)의 value는 전부 1\n",
    "    - select_probabilities의 sum == dictionary의 길이\n",
    "    - select_probabilities : 1/n (uniform distribution)\n",
    "    - selected_node : uniform distribution 하에서 random node choice\n",
    "\n",
    "- sequence 목록의 마지막 원소(path_dict[node][-1])가 0인 경우(의미 없는 sequence)\n",
    "    - 이후로 계속 0 채움(zero padding)\n",
    "\n",
    "- walk length!=0\n",
    "    - `find_next_node`함수에\n",
    "        - 'previous_node' = 마지막에서 두번째로 추가된 노드\n",
    "            - 'previous_node'와 같은 노드가 아닌 경우만 후보군에 추가\n",
    "        - 'current_node' = 가장 마지막에 추가된 노드\n",
    "        - RETURN_PARAMS = return_params(임의로 지정하는 hyperparameter)/10\n",
    "            - 'previous_node'로 설정된 값이 다시 next_node로 등장할 확률\n",
    "            - '1'인 경우에 next_node=previous_node\n",
    "    - next_node가 sequence list에 이미 존재하는지 확인\n",
    "        - 있으면 threshold +1\n",
    "            - threshold가 10을 넘으면 zero_padding 시작\n",
    "            - 중복이 10번 생기면 zero_padding\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Genertaing random walk sequence...: 100%|██████████| 10000/10000 [00:06<00:00, 1621.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.18858003616333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# origin\n",
    "start = time.time()\n",
    "walk_length=5\n",
    "all_path_list = []\n",
    "for node in tqdm(anchor_nodes, desc='Genertaing random walk sequence...'):\n",
    "    path_dict = {}\n",
    "    path_dict[node] = [node]\n",
    "    wl = 0\n",
    "    threshold = 0\n",
    "    # walk_length(sequence 길이)만큼 채울때 까지\n",
    "    while wl<walk_length-1:\n",
    "        if wl==0:\n",
    "            next_node=find_next_node(social_graph, None, node, 0.0) # walk length=0\n",
    "            path_dict[node].append(next_node)\n",
    "        elif path_dict[node][-1]==0:\n",
    "            path_dict[node].append(0)\n",
    "        else:\n",
    "            next_node=find_next_node(social_graph, path_dict[node][-2], path_dict[node][-1], 0.1) # walk length=0\n",
    "            if next_node in path_dict[node]:\n",
    "                threshold+=1\n",
    "                if threshold>10: # 10번의 patience를 주고도 중복 node가 또 나온다면(==loop 안에서 돌고 있음), 그냥 zero padding시작\n",
    "                    path_dict[node].append(0)\n",
    "                else:\n",
    "                    continue # threshold를 넘지 않을 경우, 추가하지 않고 다음 노드를 다시 찾는 작업으로 돌아감\n",
    "            else:\n",
    "                path_dict[node].append(next_node)\n",
    "        wl+=1\n",
    "\n",
    "    # # Get each user's degree information from degree table.\n",
    "    degree_list = []\n",
    "    for node_list in path_dict.values():\n",
    "        for node in node_list:\n",
    "            if node != 0:\n",
    "                degree = user_degree['degree'].loc[user_degree['user_id'] == node].values[0]\n",
    "            else:\n",
    "                # If node is 0 (zero-padded value), returns 0.\n",
    "                degree = 0\n",
    "            degree_list.append(degree)\n",
    "\n",
    "    path_dict = {key: [value, degree_list] for key, value in path_dict.items()}\n",
    "    all_path_list.append(path_dict)\n",
    "\n",
    "keys, walks, degrees = [], [], []\n",
    "for paths in all_path_list:\n",
    "    for key, value in paths.items():\n",
    "        keys.append(key)\n",
    "        walks.append(value[0])\n",
    "        degrees.append(value[1])\n",
    "\n",
    "result_df = pd.DataFrame({\n",
    "    'user_id':keys,\n",
    "    'random_walk_seq':walks,\n",
    "    'degree':degrees\n",
    "})\n",
    "result_df.sort_values(by=['user_id'], inplace=True)\n",
    "result_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Genertaing random walk sequence...: 100%|██████████| 10000/10000 [00:01<00:00, 7787.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2973229885101318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# revised - dataframe indexing(x) / hashing(o)\n",
    "# 10000개 데이터 기준, 약 5초 빠름\n",
    "start = time.time()\n",
    "walk_length=5\n",
    "anchor_seq_degree = []\n",
    "user_degree_dic = dict(zip(user_degree.user_id, user_degree.degree))\n",
    "for node in tqdm(anchor_nodes, desc='Genertaing random walk sequence...'):\n",
    "    seqs = [node]\n",
    "    wl = 0\n",
    "    threshold = 0\n",
    "    # walk_length(sequence 길이)만큼 채울때 까지\n",
    "    while wl<walk_length-1:\n",
    "        if wl==0:\n",
    "            next_node=find_next_node(social_graph, None, node, 0.0) # walk length=0\n",
    "            seqs.append(next_node)\n",
    "        elif seqs[-1]==0:\n",
    "            seqs.append(0)\n",
    "        else:\n",
    "            next_node=find_next_node(social_graph, seqs[-2], seqs[-1], 0.1) # walk length=0\n",
    "            if next_node in seqs:\n",
    "                threshold+=1\n",
    "                if threshold>10: # 10번의 patience를 주고도 중복 node가 또 나온다면(==loop 안에서 돌고 있음), 그냥 zero padding시작\n",
    "                    seqs.append(0)\n",
    "                else:\n",
    "                    continue # threshold를 넘지 않을 경우, 추가하지 않고 다음 노드를 다시 찾는 작업으로 돌아감\n",
    "            else:\n",
    "                seqs.append(next_node)\n",
    "        wl+=1\n",
    "    degrees = [0 if node==0 else user_degree_dic[node] for node in seqs]\n",
    "    anchor_seq_degree.append([node,seqs,degrees])\n",
    "random_walk_df = pd.DataFrame(anchor_seq_degree,columns=['user_id','random_walk_seq','degree'])\n",
    "random_walk_df.sort_values(by='user_id',inplace=True)\n",
    "random_walk_df.reset_index(drop=True, inplace=True)\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from ast import literal_eval\n",
    "# generate_input_sequence_data\n",
    "'''\n",
    "user_path : user sequence file dir\n",
    "item_path : user item interaction file dir(user-item-rating-degree)\n",
    "spd_path : shortest path between users file dir\n",
    "\n",
    "ast.literal_eval : data load시에, list형태로 저장된 df 원소는 str로 저장되므로, 이를 변환하기 위해 사용\n",
    "'''\n",
    "user_df = random_walk_df \n",
    "# 저장 후 불러올때 사용\n",
    "# user_df['random_walk_seq'] = user_df['random_walk_seq'].map(literal_eval)\n",
    "# user_df['degree'] = user_df['degree'].map(literal_eval)\n",
    "item_df = user_item_df\n",
    "# item_df['product_id'] = item_df['product_id'].map(literal_eval)\n",
    "# item_df['rating'] = item_df['rating'].map(literal_eval)\n",
    "# item_df['product_degree'] = item_df['product_degree'].map(literal_eval)\n",
    "spd_table = torch.from_numpy(np.load(os.path.join(ciao_dir, 'shortest_path_result.npy')))\n",
    "# rating_matrix = np.load(os.path.join(ciao_dir, 'rating_matrix.npy'))\n",
    "rating_matrix = rating_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = user_df.iloc[0]\n",
    "current_user = row['user_id']\n",
    "current_sequence = row['random_walk_seq']\n",
    "current_degree = row['degree']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2, 2, 2],\n",
       "        [1, 0, 2, 2, 2],\n",
       "        [2, 2, 0, 2, 2],\n",
       "        [2, 2, 2, 0, 3],\n",
       "        [2, 2, 2, 3, 0]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spd_table[torch.LongTensor(current_sequence).squeeze()-1,:][:,torch.LongTensor(current_sequence).squeeze()-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spd matrix의 경우에는 index가 0부터 시작하기 때문에 user index-1이 필요하고\n",
    "\n",
    "rating matrix는 user index가 1부터 시작하기 때문에 user index 그대로 사용\n",
    "\n",
    "통일됐으면 좋겠음..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:12<00:00, 813.29it/s]\n"
     ]
    }
   ],
   "source": [
    "total_df = pd.DataFrame(columns=['user_id', 'user_sequences', 'user_degree', 'item_sequences', 'item_degree', 'item_rating', 'spd_matrix'])\n",
    "# random walk sequence 한 row씩 loop\n",
    "for _,data in tqdm(user_df.iterrows(), total=user_df.shape[0]):\n",
    "    current_user = data['user_id'] \n",
    "    current_sequence = data['random_walk_seq']\n",
    "    current_degree = data['degree']\n",
    "\n",
    "    item_indexer = [int(x) for x in current_sequence] # 하나의 anchor에 해당하는 rw sequence\n",
    "    item_list, degree_list = [],[]\n",
    "    user_item_list = []\n",
    "\n",
    "    # 생성된 sequence에 해당하는 product_id, product_degree, user_id*(product seq 개수만큼)\n",
    "    \n",
    "\n",
    "    # 전체 item, degree 뽑아내서 중복 제거 후, dictionary로 만든 후에 매핑 시키는 작업\n",
    "    # 비효율적\n",
    "    for index in item_indexer:\n",
    "        if not index:\n",
    "            continue\n",
    "        # extend : ndarray가 아니라, 1d-array로 원소 붙여버림\n",
    "        item_list.extend(item_df.loc[item_df['user_id']==index, 'product_id'].values[0])\n",
    "        degree_list.extend(item_df.loc[item_df['user_id']==index, 'product_degree'].values[0])\n",
    "\n",
    "    item_list_removed_duplicate = list(set(item_list))\n",
    "\n",
    "    mapping_dict = {}\n",
    "    for item,degree in zip(item_list, degree_list):\n",
    "        if item not in mapping_dict:\n",
    "            mapping_dict[item]=degree\n",
    "\n",
    "    degree_list_removed_duplicate = [mapping_dict[item] for item in item_list_removed_duplicate]\n",
    "\n",
    "    # 사용하지도 않음(없어도 되는 코드)\n",
    "    user_mapping_dict = {}\n",
    "    for item, user in zip(item_list, user_item_list):\n",
    "        if item not in user_mapping_dict:\n",
    "            user_mapping_dict[item] = user\n",
    "\n",
    "    # padding n slicing(item & degree each)\n",
    "    sliced_item_list, num_slices = slice_and_pad_list(item_list_removed_duplicate, slice_length=item_seq_len)\n",
    "    sliced_degree_list, num_slices = slice_and_pad_list(degree_list_removed_duplicate, slice_length=item_seq_len)\n",
    "\n",
    "    # shortest path matrix에서 현재 sequence에 해당하는 부분에 해당하는 nxn matrix가져오기\n",
    "    spd_matrix = spd_table[torch.LongTensor(current_sequence).squeeze() - 1, :][:, torch.LongTensor(current_sequence).squeeze() - 1]\n",
    "\n",
    "    # "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/moon/anaconda3/envs/graphormer/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset\n",
    "# train data load -> 3m 29s\n",
    "data = pd.read_pickle('/home/moon/SocialTransformer/dataset/ciao/sequence_data_seed_42_walk_30_itemlen_100_rp_1_train.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_sequences</th>\n",
       "      <th>user_degree</th>\n",
       "      <th>item_sequences</th>\n",
       "      <th>item_degree</th>\n",
       "      <th>user_item_sequences</th>\n",
       "      <th>item_rating</th>\n",
       "      <th>spd_matrix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[1, 16, 494, 2988, 2120, 1372, 4982, 1250, 141...</td>\n",
       "      <td>[128, 227, 16, 6, 10, 346, 3, 170, 182, 9, 56,...</td>\n",
       "      <td>[24576, 1, 2, 3, 4, 5, 6, 11398, 8, 9, 10, 11,...</td>\n",
       "      <td>[1, 1, 1, 1, 3, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[260, 1, 1, 1, 1, 1, 1, 112, 1, 1, 1, 1, 112, ...</td>\n",
       "      <td>[[tensor(0), tensor(3), tensor(4), tensor(4), ...</td>\n",
       "      <td>[[tensor(0), tensor(1), tensor(2), tensor(3), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[1, 16, 494, 2988, 2120, 1372, 4982, 1250, 141...</td>\n",
       "      <td>[128, 227, 16, 6, 10, 346, 3, 170, 182, 9, 56,...</td>\n",
       "      <td>[100, 101, 11416, 103, 104, 105, 57449, 8295, ...</td>\n",
       "      <td>[9, 3, 1, 11, 1, 2, 4, 18, 1, 1, 2, 1, 1, 1, 1...</td>\n",
       "      <td>[1, 1, 112, 1, 1, 1, 1372, 294, 112, 1, 1, 260...</td>\n",
       "      <td>[[tensor(3), tensor(5), tensor(0), tensor(4), ...</td>\n",
       "      <td>[[tensor(0), tensor(1), tensor(2), tensor(3), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>[1, 16, 494, 2988, 2120, 1372, 4982, 1250, 141...</td>\n",
       "      <td>[128, 227, 16, 6, 10, 346, 3, 170, 182, 9, 56,...</td>\n",
       "      <td>[200, 201, 202, 203, 204, 11438, 206, 207, 208...</td>\n",
       "      <td>[1, 1, 1, 2, 1, 4, 1, 1, 2, 11, 4, 1, 2, 1, 1,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 112, 1, 1, 1, 260, 1, 1, 260, ...</td>\n",
       "      <td>[[tensor(4), tensor(5), tensor(5), tensor(5), ...</td>\n",
       "      <td>[[tensor(0), tensor(1), tensor(2), tensor(3), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>[1, 16, 494, 2988, 2120, 1372, 4982, 1250, 141...</td>\n",
       "      <td>[128, 227, 16, 6, 10, 346, 3, 170, 182, 9, 56,...</td>\n",
       "      <td>[304, 24881, 307, 308, 309, 310, 311, 312, 313...</td>\n",
       "      <td>[1, 4, 1, 3, 15, 1, 4, 2, 3, 1, 1, 10, 3, 2, 1...</td>\n",
       "      <td>[1, 764, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1...</td>\n",
       "      <td>[[tensor(5), tensor(0), tensor(5), tensor(3), ...</td>\n",
       "      <td>[[tensor(0), tensor(1), tensor(2), tensor(3), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>[1, 16, 494, 2988, 2120, 1372, 4982, 1250, 141...</td>\n",
       "      <td>[128, 227, 16, 6, 10, 346, 3, 170, 182, 9, 56,...</td>\n",
       "      <td>[414, 415, 416, 417, 418, 419, 421, 422, 423, ...</td>\n",
       "      <td>[1, 2, 3, 2, 3, 1, 1, 3, 1, 2, 1, 1, 1, 8, 2, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 764, 1, 1, 1, 1, 1...</td>\n",
       "      <td>[[tensor(5), tensor(5), tensor(5), tensor(1), ...</td>\n",
       "      <td>[[tensor(0), tensor(1), tensor(2), tensor(3), ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                                     user_sequences  \\\n",
       "0        1  [1, 16, 494, 2988, 2120, 1372, 4982, 1250, 141...   \n",
       "1        1  [1, 16, 494, 2988, 2120, 1372, 4982, 1250, 141...   \n",
       "2        1  [1, 16, 494, 2988, 2120, 1372, 4982, 1250, 141...   \n",
       "3        1  [1, 16, 494, 2988, 2120, 1372, 4982, 1250, 141...   \n",
       "4        1  [1, 16, 494, 2988, 2120, 1372, 4982, 1250, 141...   \n",
       "\n",
       "                                         user_degree  \\\n",
       "0  [128, 227, 16, 6, 10, 346, 3, 170, 182, 9, 56,...   \n",
       "1  [128, 227, 16, 6, 10, 346, 3, 170, 182, 9, 56,...   \n",
       "2  [128, 227, 16, 6, 10, 346, 3, 170, 182, 9, 56,...   \n",
       "3  [128, 227, 16, 6, 10, 346, 3, 170, 182, 9, 56,...   \n",
       "4  [128, 227, 16, 6, 10, 346, 3, 170, 182, 9, 56,...   \n",
       "\n",
       "                                      item_sequences  \\\n",
       "0  [24576, 1, 2, 3, 4, 5, 6, 11398, 8, 9, 10, 11,...   \n",
       "1  [100, 101, 11416, 103, 104, 105, 57449, 8295, ...   \n",
       "2  [200, 201, 202, 203, 204, 11438, 206, 207, 208...   \n",
       "3  [304, 24881, 307, 308, 309, 310, 311, 312, 313...   \n",
       "4  [414, 415, 416, 417, 418, 419, 421, 422, 423, ...   \n",
       "\n",
       "                                         item_degree  \\\n",
       "0  [1, 1, 1, 1, 3, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "1  [9, 3, 1, 11, 1, 2, 4, 18, 1, 1, 2, 1, 1, 1, 1...   \n",
       "2  [1, 1, 1, 2, 1, 4, 1, 1, 2, 11, 4, 1, 2, 1, 1,...   \n",
       "3  [1, 4, 1, 3, 15, 1, 4, 2, 3, 1, 1, 10, 3, 2, 1...   \n",
       "4  [1, 2, 3, 2, 3, 1, 1, 3, 1, 2, 1, 1, 1, 8, 2, ...   \n",
       "\n",
       "                                 user_item_sequences  \\\n",
       "0  [260, 1, 1, 1, 1, 1, 1, 112, 1, 1, 1, 1, 112, ...   \n",
       "1  [1, 1, 112, 1, 1, 1, 1372, 294, 112, 1, 1, 260...   \n",
       "2  [1, 1, 1, 1, 1, 112, 1, 1, 1, 260, 1, 1, 260, ...   \n",
       "3  [1, 764, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1...   \n",
       "4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 764, 1, 1, 1, 1, 1...   \n",
       "\n",
       "                                         item_rating  \\\n",
       "0  [[tensor(0), tensor(3), tensor(4), tensor(4), ...   \n",
       "1  [[tensor(3), tensor(5), tensor(0), tensor(4), ...   \n",
       "2  [[tensor(4), tensor(5), tensor(5), tensor(5), ...   \n",
       "3  [[tensor(5), tensor(0), tensor(5), tensor(3), ...   \n",
       "4  [[tensor(5), tensor(5), tensor(5), tensor(1), ...   \n",
       "\n",
       "                                          spd_matrix  \n",
       "0  [[tensor(0), tensor(1), tensor(2), tensor(3), ...  \n",
       "1  [[tensor(0), tensor(1), tensor(2), tensor(3), ...  \n",
       "2  [[tensor(0), tensor(1), tensor(2), tensor(3), ...  \n",
       "3  [[tensor(0), tensor(1), tensor(2), tensor(3), ...  \n",
       "4  [[tensor(0), tensor(1), tensor(2), tensor(3), ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = data.iloc[:5]\n",
    "batch = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch['user_sequence'] = torch.tensor(np.stack(sample['user_sequences']))\n",
    "batch['user_degree'] = torch.tensor(np.stack(sample['user_degree']))\n",
    "batch['item_sequence'] = torch.tensor(np.stack(sample['item_sequences']))\n",
    "batch['item_degree'] = torch.tensor(np.stack(sample['item_degree']))\n",
    "batch['user_item_seq'] = torch.tensor(np.stack(sample['user_item_sequences']))\n",
    "batch['item_rating'] = torch.tensor(np.stack(sample['item_rating']))\n",
    "batch['spd_matrix'] = torch.tensor(np.stack(sample['spd_matrix']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder\n",
    "0. user embedding vector 생성\n",
    "    - user index별 embedding + user degree별 embedding\n",
    "        - Element wise sum\n",
    "1. user sequence padding mask & Attention bias \n",
    "    - mask\n",
    "        - seq==0 인 경우 -10000으로 마스킹\n",
    "            - 근데, embedding table 생성할때 `padding_idx=0`으로 설정한거 같은데 굳이 -10000으로 바꿀 필요가 있나?\n",
    "    - attn_bias\n",
    "        - user node간의 최소 거리를 나타내는 distance matrix 중에서 사용되는 user_sequence에 해당되는 distance matrix만을 추려냄\n",
    "            - [bs=5, n_user=30]이라면, 5x30x30의 형태를 가짐\n",
    "        - 기준이 되는 user로 부터 distance가 클수록 영향력을 작게 설정하기 위해 제곱의 값에 역수를 취함\n",
    "            - `attn_bias = torch.where(attn_bias=0, 1.0, 1/(attn_bias**2).double())`\n",
    "        - 추후 계산되는 scaled dot product값과 mse_loss를 계산\n",
    "2. Multi head Attention\n",
    "    - Head split\n",
    "        - 지정된 head의 개수만큼 tensor를 나눔\n",
    "            - $5 \\times 30 \\times 64 \\rightarrow 5\\times 4 \\times 30 \\times 16$\n",
    "    - Scaled Dot Product 계산\n",
    "        - $\\frac{QK^{T}}{\\sqrt{d_k}}$\n",
    "            - $Q : 5\\times 4 \\times 30 \\times 16$\n",
    "            - $K^{T} : 5\\times 4 \\times 16 \\times 30$\n",
    "            - $\\text{output} : 5\\times 4 \\times 30 \\times 30$\n",
    "        - 계산된 dot product값은 $V$와 연산되어 최종 Attention output이 됨\n",
    "            - 각 user마다 계산된 weight에 대한 V벡터의 weighted Sum\n",
    "            - $\\text{Scaled Dot Product} : 5\\times 4 \\times 30 \\times 30$\n",
    "            - $V : 5\\times 4 \\times 30 \\times 16$\n",
    "            - $\\text{output} : 5\\times 4 \\times 30 \\times 16$\n",
    "3. FFN\n",
    "    - secondary loss(\\w score&attn_bias) layer마다 append\n",
    "4. encoder output, loss 평균값 return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset': {'train': 53393, 'dev': 8489, 'test': 8428},\n",
       " 'model': {'num_user': 7317,\n",
       "  'max_degree_user': 804,\n",
       "  'num_item': 105114,\n",
       "  'max_degree_item': 915,\n",
       "  'max_spd_value': 15,\n",
       "  'd_model': 64,\n",
       "  'd_ffn': 256,\n",
       "  'num_heads': 4,\n",
       "  'dropout': 0.1,\n",
       "  'num_layers_enc': 2,\n",
       "  'num_layers_dec': 2},\n",
       " 'training': {'batch_size': 128,\n",
       "  'optimizer': 'adamw',\n",
       "  'learning_rate': 0.0001,\n",
       "  'warmup': 40,\n",
       "  'lr_decay': 'linear',\n",
       "  'weight_decay': 0,\n",
       "  'num_epochs': 100,\n",
       "  'patience': 10,\n",
       "  'alpha': 1,\n",
       "  'beta': 3,\n",
       "  'gamma': 3}}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from config import ciao as ciao_config\n",
    "ciao_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 30, 64])\n"
     ]
    }
   ],
   "source": [
    "# 1. User Embedding vector 생성\n",
    "n_user = ciao_config['model']['num_user']\n",
    "d_model = ciao_config['model']['d_model']\n",
    "user_table = nn.Embedding(n_user+1, d_model, padding_idx=0)\n",
    "\n",
    "max_user_degree = ciao_config['model']['max_degree_user']\n",
    "user_degree_table = nn.Embedding(max_user_degree+1, d_model, padding_idx=0)\n",
    "\n",
    "user_emb = user_table(batch['user_sequence'])\n",
    "user_degree_emb = user_table(batch['user_degree'])\n",
    "user_input_emb = user_emb+user_degree_emb\n",
    "print(user_input_emb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 1, 30])\n",
      "torch.Size([5, 30, 30])\n"
     ]
    }
   ],
   "source": [
    "# 2. generate attention padding mask\n",
    "batch_size, len_seq = batch['user_sequence'].size()\n",
    "\n",
    "pad_attn_mask = (batch['user_sequence'].data!=0).unsqueeze(1) # bs x 1 x len_user_seq\n",
    "print(pad_attn_mask.shape)\n",
    "pad_attn_mask = pad_attn_mask.expand(batch_size, len_seq, len_seq) # bs x len_user_seq x len_user_seq\n",
    "print(pad_attn_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 30, 30])\n",
      "torch.Size([5, 30, 30, 4])\n"
     ]
    }
   ],
   "source": [
    "# 3. spatial encoding(Spatial Encoder) -> spatial & positional bias\n",
    "num_heads = ciao_config['model']['num_heads']\n",
    "# bs x len_user_seq x len_user_seq -> n_head x bs x len_user_seq x len_user_seq -> bs x len_user_seq x len_user_seq x num_heads\n",
    "print(batch['spd_matrix'].shape)\n",
    "spatial_bias = batch['spd_matrix'].repeat(num_heads, 1, 1, 1).permute(1,2,3,0)\n",
    "print(spatial_bias.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before split : torch.Size([5, 30, 64])\n",
      "after split : torch.Size([5, 4, 30, 16])\n",
      "mask shape before : torch.Size([5, 30, 30])\n",
      "mask shape after: torch.Size([5, 4, 30, 30])\n",
      "attn bias shape : torch.Size([5, 4, 30, 30])\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Multi-head Attention\n",
    "\n",
    "W_Q = nn.Linear(d_model,d_model)\n",
    "W_K = nn.Linear(d_model,d_model)\n",
    "W_V = nn.Linear(d_model,d_model)\n",
    "\n",
    "Q, K, V = W_Q(user_input_emb), W_K(user_input_emb), W_V(user_input_emb)\n",
    "print(\"before split :\", Q.shape)\n",
    "# split q,k,v vector by 'num_heads'\n",
    "def split_tensor(tensor, n_head=4):\n",
    "    bs, length, d_model = tensor.size()\n",
    "    d_tensor = d_model // n_head\n",
    "    tensor = tensor.view(bs, n_head, length, d_tensor)\n",
    "    return tensor\n",
    "\n",
    "Q, K, V = split_tensor(Q), split_tensor(K), split_tensor(V)\n",
    "print(\"after split :\", Q.shape) # bs x len_user_seq x n_head x d\n",
    "mask = pad_attn_mask # bs x len_user_seq x len_user_seq\n",
    "print(\"mask shape before :\", mask.shape)\n",
    "if mask!=None:\n",
    "    mask = mask.unsqueeze(1).repeat(1, num_heads, 1, 1)\n",
    "    print(\"mask shape after:\",mask.shape)\n",
    "attn_bias = spatial_bias.permute(0,3,2,1)\n",
    "print(\"attn bias shape :\", attn_bias.shape)\n",
    "spd_param = nn.Parameter(torch.randn((30,30), dtype=torch.float), requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 30, 64])\n"
     ]
    }
   ],
   "source": [
    "# Scaled Dot Product\n",
    "d_tensor = K.size()[-1]\n",
    "K_T = K.transpose(2,3)\n",
    "score = torch.matmul(Q, K_T) / math.sqrt(d_tensor)\n",
    "\n",
    "# mask(user_seq이 0인 경우에 False)\n",
    "if mask!=None:\n",
    "    score = score.masked_fill(mask==0, -10000) # mask(seq==0)인 경우 dot product값을 -10000으로 치환\n",
    "    # 이 경우에, attn_bias값과 차이가 심할텐데, loss반영이 잘 되는지...?\n",
    "    # 1/1e9로 치환하는게 낫지 않을까?\n",
    "if attn_bias!=None:\n",
    "    attn_bias = torch.where(attn_bias==0.0, 1.0, (1/(attn_bias)**2).double())\n",
    "    loss = torch.sqrt(F.mse_loss(score.float(), attn_bias.float())) # attention 결과와 attn bias의 mse loss?\n",
    "\n",
    "score = torch.softmax(score, dim=-1) # dim = -1 : sequence별 softmax\n",
    "V = torch.matmul(score,V)\n",
    "# concat\n",
    "bs, n_head, length, d_tensor = V.size()\n",
    "V = V.transpose(1,2).contiguous().view(bs, length, n_head*d_tensor)\n",
    "W_concat = nn.Linear(d_model,d_model)\n",
    "encoder_output = W_concat(V)\n",
    "print(encoder_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 제대로 concat되는지 확인(transpose후 view가 맞음)\n",
    "# t = torch.randn([2,2,3,2])\n",
    "# t_2 = t.transpose(1,2).contiguous().view(2,3,4)\n",
    "# print(t_2[0][0])\n",
    "# t_3 = t.reshape(2,3,4)\n",
    "# print(t_3[0][0])\n",
    "# # head별로 찢고 concat\n",
    "# t_4 = torch.cat([t[:,0,:], t[:,1,:]],dim=-1)\n",
    "# print(t_4[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- score\n",
    "    - Scaled dot product값\n",
    "        - 각 sequence에 대한 가중치\n",
    "- attn_bias\n",
    "    - shortest distance\n",
    "        - 전체 sequence 내에서 user별로 타 user간의 최소 거리 행렬을 나타낸 값의 제곱의 역수\n",
    "        - 거리가 멀수록 값이 작고, 거리가 가까울 수록 값이 큼\n",
    "- loss\n",
    "    - score, attn_bias의 mse loss\n",
    "    - 학습을 통해 attention 결과와 attn_bias를 가깝게 만들어주기 위함\n",
    "- OK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Multi head Concat시 torch 사용법\n",
    "    - 단순히 reshape or view를 하게 되면, 제대로 concatenation이 일어나지 않음\n",
    "    1. transpose해서 head와 length의 dimension을 바꿈\n",
    "    2. contiguous하게 만들기\n",
    "    3. reshape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi head Attention 과정\n",
    "1. Q,K,V 벡터 생성\n",
    "2. sequence mask & attn bias 생성\n",
    "    - mask : seq==0인경우 -10000으로 치환\n",
    "    - attn_bias : score(scaled dot product)와 비교해 loss 계산\n",
    "3. Scaled Dot Product 계산\n",
    "4. Scaled Dot product와 attn_bias와의 loss 계산\n",
    "5. Attention output(V) return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['user_sequence', 'user_degree', 'item_sequence', 'item_degree', 'user_item_seq', 'item_rating', 'spd_matrix'])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.keys() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 100, 64])\n"
     ]
    }
   ],
   "source": [
    "# Item embedding(ItemNodeEncoder)\n",
    "num_item = ciao_config['model']['num_item']\n",
    "max_item_degree = ciao_config['model']['max_degree_item']\n",
    "item_table = nn.Embedding(num_item+1, d_model, padding_idx=0)\n",
    "item_degree_table = nn.Embedding(max_item_degree+1, d_model, padding_idx=0)\n",
    "item_embedding = item_table(batch['item_sequence'])+item_degree_table(batch['item_degree'])\n",
    "print(item_embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 100, 100])\n",
      "torch.Size([5, 100, 30])\n"
     ]
    }
   ],
   "source": [
    "# self attention mask\n",
    "bs,len_item = batch['item_sequence'].size()\n",
    "dec_self_attn_mask = (batch['item_sequence']!=0).unsqueeze(1).expand(-1,len_item,-1)\n",
    "print(dec_self_attn_mask.shape)\n",
    "# cross attention mask\n",
    "bs,len_item = batch['item_sequence'].size()\n",
    "bs,len_user = batch['user_sequence'].size()\n",
    "dec_cross_attn_mask = (batch['user_sequence']!=0).unsqueeze(1).expand(-1,len_item,len_user)\n",
    "print(dec_cross_attn_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 4, 100, 30])\n"
     ]
    }
   ],
   "source": [
    "# attention bias(item rating - implicit rating)\n",
    "item_rating = torch.where(batch['item_rating']==0,0,1)\n",
    "bs,len_user,len_item = item_rating.size()\n",
    "decoder_attn_bias = item_rating.repeat(num_heads,1,1,1).permute(1,2,3,0).permute(0,3,2,1) # 원본\n",
    "print(decoder_attn_bias.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deocder Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 100, 100])\n",
      "torch.Size([5, 4, 100, 100])\n",
      "torch.Size([5, 4, 100, 100])\n",
      "torch.Size([5, 4, 100, 100])\n",
      "torch.Size([5, 4, 100, 16])\n"
     ]
    }
   ],
   "source": [
    "# 1. self attention(decoder-decoder / attn_bias (x))\n",
    "W_q, W_k, W_v  = nn.Linear(d_model,d_model), nn.Linear(d_model,d_model), nn.Linear(d_model,d_model)\n",
    "Q,K,V = W_q(item_embedding), W_k(item_embedding), W_v(item_embedding)\n",
    "Q,K,V = split_tensor(Q), split_tensor(K), split_tensor(V)\n",
    "if dec_self_attn_mask!=None:\n",
    "    print(dec_self_attn_mask.shape)\n",
    "    # head수 만큼 차원 확장\n",
    "    dec_self_attn_mask = dec_self_attn_mask.unsqueeze(1).expand(-1,num_heads,-1,-1)\n",
    "    print(dec_self_attn_mask.shape)\n",
    "    \n",
    "# scaled dot product\n",
    "bs, h, len_item, d_tensor = Q.shape\n",
    "K_T = K.transpose(-2,-1)\n",
    "score = torch.matmul(Q,K_T) / math.sqrt(d_model)\n",
    "print(score.shape)\n",
    "\n",
    "if dec_self_attn_mask != None:\n",
    "    score = score.masked_fill(dec_self_attn_mask==0,-10000)\n",
    "    \n",
    "loss = 0\n",
    "# attn_bias==None(self attention)\n",
    "score = torch.softmax(score,dim=-1)\n",
    "print(score.shape)\n",
    "V = torch.matmul(score,V)\n",
    "print(V.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 100, 30])\n",
      "torch.Size([5, 4, 100, 30])\n",
      "torch.Size([5, 4, 100, 30])\n",
      "tensor(0.9198, grad_fn=<MeanBackward0>)\n",
      "torch.Size([5, 4, 100, 16])\n"
     ]
    }
   ],
   "source": [
    "# 2. cross attention(encoder-decoder / attn_bias(o))\n",
    "# Q-decoder / K,V - encoder\n",
    "W_q, W_k, W_v  = nn.Linear(d_model,d_model), nn.Linear(d_model,d_model), nn.Linear(d_model,d_model)\n",
    "Q,K,V = W_q(item_embedding), W_k(encoder_output), W_v(encoder_output)\n",
    "Q,K,V = split_tensor(Q), split_tensor(K), split_tensor(V)\n",
    "\n",
    "if dec_cross_attn_mask!=None:\n",
    "    print(dec_cross_attn_mask.shape)\n",
    "    # head수 만큼 차원 확장\n",
    "    dec_cross_attn_mask = dec_cross_attn_mask.unsqueeze(1).expand(-1,num_heads,-1,-1)\n",
    "    print(dec_cross_attn_mask.shape)\n",
    "    \n",
    "# scaled dot product\n",
    "bs, h, len_item, d_tensor = Q.shape\n",
    "bs, h, len_user, d_tensor = K.shape\n",
    "K_T = K.transpose(-2,-1)\n",
    "score = torch.matmul(Q,K_T) / math.sqrt(d_model)\n",
    "print(score.shape)\n",
    "# zero padding -> masking 적용(0인 sequence에 대한 tensor값 -10000로 치환)\n",
    "if dec_cross_attn_mask!=None:\n",
    "    score = score.masked_fill(dec_cross_attn_mask==0, -10000)\n",
    "    \n",
    "if decoder_attn_bias!=None:\n",
    "    decoder_attn_bias = torch.where(decoder_attn_bias==0,-1,1)\n",
    "    loss = torch.mean(torch.abs((torch.sign(score.float())-torch.sign(decoder_attn_bias.float()))))\n",
    "    print(loss)\n",
    "\n",
    "score = torch.softmax(score,dim=-1)\n",
    "V = torch.matmul(score,V)\n",
    "print(V.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_bias))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- attn_bias(Decoder)\n",
    "    - Decoder에서 계산되는 attention(cross)에 대해서 loss 계산(MAE)\n",
    "        - cross attention\n",
    "            - user & item representation에 대한 연산\n",
    "            - bs x head x n_user x n_item(?)\n",
    "    - self-attetion에 대해서는 attn_bias 없음"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphormer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7a97eeda1d14659ef3e54d6c184d8a253d77a00f423002607462fe8c2d8c6cef"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
